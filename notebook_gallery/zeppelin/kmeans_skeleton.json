{"paragraphs":[{"text":"\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\n\nval df = sqlContext.createDataFrame(Seq(\n  (0, \"a\", \"Desktop\", 2, 0.1),\n  (1, \"b\", \"Mobile\", 2, 0.0),\n  (2, \"c\", \"\", 3, 0.5),\n  (3, \"\", \"appleTV\", 3, 0.77),\n  (4, \"a\", \"Desktop\", 2, 0.0),\n  (5, \"c\", \"Mobile\", 2, 0.2)\n)).toDF(\"user_id\", \"category\", \"device\", \"channel_id\", \"tvm\")\n\n\nval temp = df.na.replace(Seq(\"category\", \"device\"), Map(\"\" -> \"NA\"))\ntemp.show()\n\n// To handle categorical parameters\nval categorical_features = Array(\"category\", \"device\")\nval indexer: Array[org.apache.spark.ml.PipelineStage] = categorical_features.map(\n    fName => new StringIndexer()\n    .setInputCol(fName)\n    .setOutputCol(s\"${fName}Index\"))\nval index_pipeline = new Pipeline().setStages(indexer)\nval index_model = index_pipeline.fit(temp)\nval dfIndexed = index_model.transform(temp)\n\n// One Hot Encoding\nval indexColumns  = dfIndexed.columns.filter(x => x contains \"Index\")\nval encoder:Array[org.apache.spark.ml.PipelineStage] = indexColumns.map(\n    fName => new OneHotEncoder()\n    .setInputCol(fName)\n    .setOutputCol(s\"${fName}Vec\"))\n\nval encoderPipeline = new Pipeline().setStages(encoder)\nval hotEncodedDF = encoderPipeline.fit(dfIndexed).transform(dfIndexed)\n\nval assembler = new VectorAssembler().setInputCols(Array(\"tvm\")).setOutputCol(\"features\")\nval model = new KMeans().setSeed(0).setK(5).setMaxIter(10).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\nval pipeline = new Pipeline().setStages(Array(assembler, model))\nval kMeansModelFit = pipeline.fit(hotEncodedDF)\n\nval transformedFeaturesDF = kMeansModelFit.transform(hotEncodedDF)\ntransformedFeaturesDF.show()\nval cost = kMeansModelFit.stages(1).asInstanceOf[KMeansModel].computeCost(transformedFeaturesDF)\nprintln(cost)\nprintln(\"Cluster Centers: \")\nkMeansModelFit.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)\n\n\n// val indexer = new StringIndexer().setInputCol(Array(\"category\", \"device\")).setOutputCol(Array(\"categoryIndex\", \"deviceIndex\"))\n// val encoder = new OneHotEncoder().setInputCol(Array(\"categoryIndex\", \"deviceIndex\")).setOutputCol(Array(\"categoryVec\", \"deviceVec\"))\n// val assembler = new VectorAssembler().setInputCols(Array(\"categoryIndexVec\", \"deviceIndexVec\", \"channel_id\")).setOutputCol(\"features\")\n// val model = new KMeans().setSeed(0).setK(5).setMaxIter(10).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\n// val pipeline = new Pipeline().setStages(Array(indexer ++ encoder, assembler, model))\n// val kMeansModelFit = pipeline.fit(temp)\n\n// val transformedFeaturesDF = kMeansModelFit.transform(temp)\n// transformedFeaturesDF.show()\n// val cost = kMeansModelFit.stages(3).asInstanceOf[KMeansModel].computeCost(transformedFeaturesDF)\n// println(cost)\n// println(\"Cluster Centers: \")\n// kMeansModelFit.stages(3).asInstanceOf[KMeansModel].clusterCenters.foreach(println)","dateUpdated":"2016-10-11T07:58:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476215939363_899095145","id":"20160831-224050_462455413","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\ndf: org.apache.spark.sql.DataFrame = [user_id: int, category: string ... 3 more fields]\ntemp: org.apache.spark.sql.DataFrame = [user_id: int, category: string ... 3 more fields]\n+-------+--------+-------+----------+----+\n|user_id|category| device|channel_id| tvm|\n+-------+--------+-------+----------+----+\n|      0|       a|Desktop|         2| 0.1|\n|      1|       b| Mobile|         2| 0.0|\n|      2|       c|     NA|         3| 0.5|\n|      3|      NA|appleTV|         3|0.77|\n|      4|       a|Desktop|         2| 0.0|\n|      5|       c| Mobile|         2| 0.2|\n+-------+--------+-------+----------+----+\n\ncategorical_features: Array[String] = Array(category, device)\nindexer: Array[org.apache.spark.ml.PipelineStage] = Array(strIdx_ee4be9331a32, strIdx_bc583fb85189)\nindex_pipeline: org.apache.spark.ml.Pipeline = pipeline_d2fec555fb3a\nindex_model: org.apache.spark.ml.PipelineModel = pipeline_d2fec555fb3a\ndfIndexed: org.apache.spark.sql.DataFrame = [user_id: int, category: string ... 5 more fields]\nindexColumns: Array[String] = Array(categoryIndex, deviceIndex)\nencoder: Array[org.apache.spark.ml.PipelineStage] = Array(oneHot_d36f406587c8, oneHot_c2129b1f1e62)\nencoderPipeline: org.apache.spark.ml.Pipeline = pipeline_2538daee6064\nhotEncodedDF: org.apache.spark.sql.DataFrame = [user_id: int, category: string ... 7 more fields]\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_52b99b61cf31\nmodel: org.apache.spark.ml.clustering.KMeans = kmeans_6920532bf7ef\npipeline: org.apache.spark.ml.Pipeline = pipeline_3c08e2d0e98d\nkMeansModelFit: org.apache.spark.ml.PipelineModel = pipeline_3c08e2d0e98d\ntransformedFeaturesDF: org.apache.spark.sql.DataFrame = [user_id: int, category: string ... 9 more fields]\n+-------+--------+-------+----------+----+-------------+-----------+----------------+--------------+--------+----------+\n|user_id|category| device|channel_id| tvm|categoryIndex|deviceIndex|categoryIndexVec|deviceIndexVec|features|prediction|\n+-------+--------+-------+----------+----+-------------+-----------+----------------+--------------+--------+----------+\n|      0|       a|Desktop|         2| 0.1|          0.0|        0.0|   (3,[0],[1.0])| (3,[0],[1.0])|   [0.1]|         4|\n|      1|       b| Mobile|         2| 0.0|          2.0|        1.0|   (3,[2],[1.0])| (3,[1],[1.0])|   [0.0]|         3|\n|      2|       c|     NA|         3| 0.5|          1.0|        2.0|   (3,[1],[1.0])| (3,[2],[1.0])|   [0.5]|         2|\n|      3|      NA|appleTV|         3|0.77|          3.0|        3.0|       (3,[],[])|     (3,[],[])|  [0.77]|         1|\n|      4|       a|Desktop|         2| 0.0|          0.0|        0.0|   (3,[0],[1.0])| (3,[0],[1.0])|   [0.0]|         3|\n|      5|       c| Mobile|         2| 0.2|          1.0|        1.0|   (3,[1],[1.0])| (3,[1],[1.0])|   [0.2]|         0|\n+-------+--------+-------+----------+----+-------------+-----------+----------------+--------------+--------+----------+\n\ncost: Double = 0.0\n0.0\nCluster Centers: \n[0.2]\n[0.77]\n[0.5]\n[0.0]\n[0.1]\n"},"dateCreated":"2016-10-11T07:58:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3070"},{"title":"Clustering on channel_category","text":"import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\n\nval df = sqlContext.createDataFrame(Seq(\n  (0, \"a, b, c\"),\n  (1, \"a, b, b c, a\"),\n  (2, \"a, e, a\"),\n  (3, \"e, f\"),\n  (4, \"a, e, c\"),\n  (5, \"b, a, c\")\n)).toDF(\"user_id\", \"channels\")\n\n\n// To handle categorical parameters\nval categorical_features = Array(\"channels\")\nval indexer: Array[org.apache.spark.ml.PipelineStage] = categorical_features.map(\n    fName => new StringIndexer()\n    .setInputCol(fName)\n    .setOutputCol(s\"${fName}Index\"))\nval index_pipeline = new Pipeline().setStages(indexer)\nval index_model = index_pipeline.fit(df)\nval dfIndexed = index_model.transform(df)\n\n// One Hot Encoding\nval indexColumns  = dfIndexed.columns.filter(x => x contains \"Index\")\nval encoder:Array[org.apache.spark.ml.PipelineStage] = indexColumns.map(\n    fName => new OneHotEncoder()\n    .setInputCol(fName)\n    .setOutputCol(s\"${fName}Vec\"))\n\nval encoderPipeline = new Pipeline().setStages(encoder)\nval hotEncodedDF = encoderPipeline.fit(dfIndexed).transform(dfIndexed)\n\nval assembler = new VectorAssembler().setInputCols(Array(\"channelsIndexVec\")).setOutputCol(\"features\")\nval model = new KMeans().setSeed(0).setK(2).setMaxIter(20).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\nval pipeline = new Pipeline().setStages(Array(assembler, model))\nval kMeansModelFit = pipeline.fit(hotEncodedDF)\nval transformedFeaturesDF = kMeansModelFit.transform(hotEncodedDF)\ntransformedFeaturesDF.show()\n\nval cost = kMeansModelFit.stages(1).asInstanceOf[KMeansModel].computeCost(transformedFeaturesDF)\nprintln(cost)\nprintln(\"Cluster Centers: \")\nkMeansModelFit.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)","dateUpdated":"2016-10-11T07:58:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476215939363_899095145","id":"20160921-223224_890492588","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\ndf: org.apache.spark.sql.DataFrame = [user_id: int, channels: string]\ncategorical_features: Array[String] = Array(channels)\nindexer: Array[org.apache.spark.ml.PipelineStage] = Array(strIdx_7d6ec77a99fc)\nindex_pipeline: org.apache.spark.ml.Pipeline = pipeline_79d0eb9e656e\nindex_model: org.apache.spark.ml.PipelineModel = pipeline_79d0eb9e656e\ndfIndexed: org.apache.spark.sql.DataFrame = [user_id: int, channels: string ... 1 more field]\nindexColumns: Array[String] = Array(channelsIndex)\nencoder: Array[org.apache.spark.ml.PipelineStage] = Array(oneHot_90fc269b27ec)\nencoderPipeline: org.apache.spark.ml.Pipeline = pipeline_ad41bdedd3ab\nhotEncodedDF: org.apache.spark.sql.DataFrame = [user_id: int, channels: string ... 2 more fields]\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_56d4045d36f5\nmodel: org.apache.spark.ml.clustering.KMeans = kmeans_a26b5642d6b3\npipeline: org.apache.spark.ml.Pipeline = pipeline_b0522ee38f21\nkMeansModelFit: org.apache.spark.ml.PipelineModel = pipeline_b0522ee38f21\ntransformedFeaturesDF: org.apache.spark.sql.DataFrame = [user_id: int, channels: string ... 4 more fields]\n+-------+------------+-------------+----------------+-------------+----------+\n|user_id|    channels|channelsIndex|channelsIndexVec|     features|prediction|\n+-------+------------+-------------+----------------+-------------+----------+\n|      0|     a, b, c|          1.0|   (5,[1],[1.0])|(5,[1],[1.0])|         1|\n|      1|a, b, b c, a|          0.0|   (5,[0],[1.0])|(5,[0],[1.0])|         0|\n|      2|     a, e, a|          5.0|       (5,[],[])|    (5,[],[])|         0|\n|      3|        e, f|          3.0|   (5,[3],[1.0])|(5,[3],[1.0])|         0|\n|      4|     a, e, c|          2.0|   (5,[2],[1.0])|(5,[2],[1.0])|         0|\n|      5|     b, a, c|          4.0|   (5,[4],[1.0])|(5,[4],[1.0])|         0|\n+-------+------------+-------------+----------------+-------------+----------+\n\ncost: Double = 3.2\n3.2\nCluster Centers: \n[0.2,0.0,0.2,0.2,0.2]\n[0.0,1.0,0.0,0.0,0.0]\n"},"dateCreated":"2016-10-11T07:58:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3071"},{"text":"sc","dateUpdated":"2016-10-11T08:01:48+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476216105924_-272836325","id":"20161011-200145_1391572596","result":{"code":"SUCCESS","type":"TEXT","msg":"res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4d924faf\n"},"dateCreated":"2016-10-11T08:01:45+0000","dateStarted":"2016-10-11T08:01:48+0000","dateFinished":"2016-10-11T08:02:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3072"},{"title":"A better way to Do K-means(ignoring the ordering of the items)","text":"import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, Tokenizer}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\n\ndef combine(a:String, b:String): String = {\n    a + \" \" + b\n}\n\nval df = sqlContext.createDataFrame(Seq(\n  (0, \"a\"),\n  (0, \"b\"),\n  (0, \"c\"),\n  (1, \"a\"), \n  (1, \"b\"), \n  (1, \"b\"),\n  (1, \"c\"), \n  (1, \"a\"),\n  (2, \"a\"), \n  (2, \"e\"), \n  (2, \"a\"),\n  (3, \"e\"), \n  (3, \"f\"),\n  (4, \"a\"), \n  (4, \"e\"), \n  (4, \"c\"),\n  (5, \"b\"), \n  (5, \"a\"), \n  (5, \"c\"),\n  (6, \"e\"),\n  (7, \"e\"),\n  (6, \"f\")\n)).toDF(\"user_id\", \"channels\")\n\nval dft = df.distinct().rdd.map(x => (x(0).toString, x(1).toString)).\n                            reduceByKey(combine(_,_)).toDF().selectExpr(\"_1 as user_id\", \"_2 as channels\")\ndft.show(false)\n\nval tokenizer = new Tokenizer().setInputCol(\"channels\").setOutputCol(\"tokenized\")\nval wordsData = tokenizer.transform(dft)\nwordsData.show(false)\n\nval cvModel: CountVectorizerModel = new CountVectorizer()\n  .setInputCol(\"tokenized\")\n  .setOutputCol(\"raw_features\")\n  .setMinDF(1)\n  .fit(wordsData)\n  \nval dataset = cvModel.transform(wordsData).select(\"user_id\", \"raw_features\")\ndataset.show(false)\n\nval inputData = dataset.rdd.map(x => (x(0).toString, x(1).toString)).toDF().selectExpr(\"_1 as user_id\", \"_2 as channels\")\ninputData.show(false)\n\n\n// To handle categorical parameters\nval categorical_features = Array(\"channels\")\nval indexer: Array[org.apache.spark.ml.PipelineStage] = categorical_features.map(\n    fName => new StringIndexer()\n    .setInputCol(fName)\n    .setOutputCol(s\"${fName}Index\"))\nval index_pipeline = new Pipeline().setStages(indexer)\nval index_model = index_pipeline.fit(inputData)\nval dfIndexed = index_model.transform(inputData)\n\n// One Hot Encoding\nval indexColumns  = dfIndexed.columns.filter(x => x contains \"Index\")\nval encoder:Array[org.apache.spark.ml.PipelineStage] = indexColumns.map(\n    fName => new OneHotEncoder()\n    .setInputCol(fName)\n    .setOutputCol(s\"${fName}Vec\"))\n\nval encoderPipeline = new Pipeline().setStages(encoder)\nval hotEncodedDF = encoderPipeline.fit(dfIndexed).transform(dfIndexed)\nval data = hotEncodedDF.select(\"user_id\", \"channelsIndexVec\")\nhotEncodedDF.show(false)\n\nval assembler = new VectorAssembler().setInputCols(Array(\"channelsIndexVec\")).setOutputCol(\"features\")\nval model = new KMeans().setSeed(0).setK(3).setMaxIter(20).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\nval pipeline = new Pipeline().setStages(Array(assembler, model))\nval kMeansModelFit = pipeline.fit(hotEncodedDF)\nval transformedFeaturesDF = kMeansModelFit.transform(hotEncodedDF)\ntransformedFeaturesDF.show()\n\nval cost = kMeansModelFit.stages(1).asInstanceOf[KMeansModel].computeCost(transformedFeaturesDF)\nprintln(cost)\nprintln(\"Cluster Centers: \")\nkMeansModelFit.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)","dateUpdated":"2016-10-11T08:05:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476215939363_899095145","id":"20161009-165303_1870476339","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, Tokenizer}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\ncombine: (a: String, b: String)String\ndf: org.apache.spark.sql.DataFrame = [user_id: int, channels: string]\ndft: org.apache.spark.sql.DataFrame = [user_id: string, channels: string]\n+-------+--------+\n|user_id|channels|\n+-------+--------+\n|0      |b c a   |\n|1      |c b a   |\n|2      |e a     |\n|3      |f e     |\n|4      |e c a   |\n|5      |a c b   |\n|6      |f e     |\n|7      |e       |\n+-------+--------+\n\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_f443adcfdb79\nwordsData: org.apache.spark.sql.DataFrame = [user_id: string, channels: string ... 1 more field]\n+-------+--------+---------+\n|user_id|channels|tokenized|\n+-------+--------+---------+\n|0      |b c a   |[b, c, a]|\n|1      |c b a   |[c, b, a]|\n|2      |e a     |[e, a]   |\n|3      |f e     |[f, e]   |\n|4      |e c a   |[e, c, a]|\n|5      |a c b   |[a, c, b]|\n|6      |f e     |[f, e]   |\n|7      |e       |[e]      |\n+-------+--------+---------+\n\ncvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_9ab291b34703\ndataset: org.apache.spark.sql.DataFrame = [user_id: string, raw_features: vector]\n+-------+-------------------------+\n|user_id|raw_features             |\n+-------+-------------------------+\n|0      |(5,[0,2,3],[1.0,1.0,1.0])|\n|1      |(5,[0,2,3],[1.0,1.0,1.0])|\n|2      |(5,[0,1],[1.0,1.0])      |\n|3      |(5,[1,4],[1.0,1.0])      |\n|4      |(5,[0,1,2],[1.0,1.0,1.0])|\n|5      |(5,[0,2,3],[1.0,1.0,1.0])|\n|6      |(5,[1,4],[1.0,1.0])      |\n|7      |(5,[1],[1.0])            |\n+-------+-------------------------+\n\ninputData: org.apache.spark.sql.DataFrame = [user_id: string, channels: string]\n+-------+-------------------------+\n|user_id|channels                 |\n+-------+-------------------------+\n|0      |(5,[0,2,3],[1.0,1.0,1.0])|\n|1      |(5,[0,2,3],[1.0,1.0,1.0])|\n|2      |(5,[0,1],[1.0,1.0])      |\n|3      |(5,[1,4],[1.0,1.0])      |\n|4      |(5,[0,1,2],[1.0,1.0,1.0])|\n|5      |(5,[0,2,3],[1.0,1.0,1.0])|\n|6      |(5,[1,4],[1.0,1.0])      |\n|7      |(5,[1],[1.0])            |\n+-------+-------------------------+\n\ncategorical_features: Array[String] = Array(channels)\nindexer: Array[org.apache.spark.ml.PipelineStage] = Array(strIdx_3271a1129cab)\nindex_pipeline: org.apache.spark.ml.Pipeline = pipeline_699df1fc889b\nindex_model: org.apache.spark.ml.PipelineModel = pipeline_699df1fc889b\ndfIndexed: org.apache.spark.sql.DataFrame = [user_id: string, channels: string ... 1 more field]\nindexColumns: Array[String] = Array(channelsIndex)\nencoder: Array[org.apache.spark.ml.PipelineStage] = Array(oneHot_f9c924f50b7c)\nencoderPipeline: org.apache.spark.ml.Pipeline = pipeline_19eb1ae7fbc6\nhotEncodedDF: org.apache.spark.sql.DataFrame = [user_id: string, channels: string ... 2 more fields]\ndata: org.apache.spark.sql.DataFrame = [user_id: string, channelsIndexVec: vector]\n+-------+-------------------------+-------------+----------------+\n|user_id|channels                 |channelsIndex|channelsIndexVec|\n+-------+-------------------------+-------------+----------------+\n|0      |(5,[0,2,3],[1.0,1.0,1.0])|0.0          |(4,[0],[1.0])   |\n|1      |(5,[0,2,3],[1.0,1.0,1.0])|0.0          |(4,[0],[1.0])   |\n|2      |(5,[0,1],[1.0,1.0])      |2.0          |(4,[2],[1.0])   |\n|3      |(5,[1,4],[1.0,1.0])      |1.0          |(4,[1],[1.0])   |\n|4      |(5,[0,1,2],[1.0,1.0,1.0])|4.0          |(4,[],[])       |\n|5      |(5,[0,2,3],[1.0,1.0,1.0])|0.0          |(4,[0],[1.0])   |\n|6      |(5,[1,4],[1.0,1.0])      |1.0          |(4,[1],[1.0])   |\n|7      |(5,[1],[1.0])            |3.0          |(4,[3],[1.0])   |\n+-------+-------------------------+-------------+----------------+\n\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e1849c0a8273\nmodel: org.apache.spark.ml.clustering.KMeans = kmeans_0428652318a4\npipeline: org.apache.spark.ml.Pipeline = pipeline_e6a35fb6168a\nkMeansModelFit: org.apache.spark.ml.PipelineModel = pipeline_e6a35fb6168a\ntransformedFeaturesDF: org.apache.spark.sql.DataFrame = [user_id: string, channels: string ... 4 more fields]\n+-------+--------------------+-------------+----------------+-------------+----------+\n|user_id|            channels|channelsIndex|channelsIndexVec|     features|prediction|\n+-------+--------------------+-------------+----------------+-------------+----------+\n|      0|(5,[0,2,3],[1.0,1...|          0.0|   (4,[0],[1.0])|(4,[0],[1.0])|         1|\n|      1|(5,[0,2,3],[1.0,1...|          0.0|   (4,[0],[1.0])|(4,[0],[1.0])|         1|\n|      2| (5,[0,1],[1.0,1.0])|          2.0|   (4,[2],[1.0])|(4,[2],[1.0])|         0|\n|      3| (5,[1,4],[1.0,1.0])|          1.0|   (4,[1],[1.0])|(4,[1],[1.0])|         0|\n|      4|(5,[0,1,2],[1.0,1...|          4.0|       (4,[],[])|    (4,[],[])|         0|\n|      5|(5,[0,2,3],[1.0,1...|          0.0|   (4,[0],[1.0])|(4,[0],[1.0])|         1|\n|      6| (5,[1,4],[1.0,1.0])|          1.0|   (4,[1],[1.0])|(4,[1],[1.0])|         0|\n|      7|       (5,[1],[1.0])|          3.0|   (4,[3],[1.0])|(4,[3],[1.0])|         2|\n+-------+--------------------+-------------+----------------+-------------+----------+\n\ncost: Double = 1.75\n1.75\nCluster Centers: \n[0.0,0.5,0.25,0.0]\n[1.0,0.0,0.0,0.0]\n[0.0,0.0,0.0,1.0]\n"},"dateCreated":"2016-10-11T07:58:59+0000","dateStarted":"2016-10-11T08:05:55+0000","dateFinished":"2016-10-11T08:06:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3073"},{"text":"// Experimental code\n/*\ninputDf.show(false)\nval tokenizer = new Tokenizer().setInputCol(\"binary_vector\").setOutputCol(\"tokenized\")\nval wordsData = tokenizer.transform(inputDf)\nval hashingTF = new HashingTF().setBinary(true).setInputCol(\"tokenized\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\nval featurizedData = hashingTF.transform(wordsData)\nfeaturizedData.select(\"rawFeatures\").show(false)\n*/\n\n/*\nval idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"extractedFeatures\")\nval idfModel = idf.fit(featurizedData)\nval rescaledData = idfModel.transform(featurizedData)\nval hashedFeatures = rescaledData\nhashedFeatures.show(false)\nhashedFeatures.printSchema()\n*/\n\n/*\nval assembler = new VectorAssembler().setInputCols(Array(\"binary_vector\")).setOutputCol(\"features\")\nval model = new KMeans().setSeed(0).setK(2).setMaxIter(20).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\nval pipeline = new Pipeline().setStages(Array(assembler, model))\nval kMeansModelFit = pipeline.fit(groups)\nval transformedFeaturesDF = kMeansModelFit.transform(groups)\ntransformedFeaturesDF.show()\n\nval cost = kMeansModelFit.stages(1).asInstanceOf[KMeansModel].computeCost(transformedFeaturesDF)\nprintln(cost)\nprintln(\"Cluster Centers: \")\nkMeansModelFit.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)\n*/","dateUpdated":"2016-10-11T07:58:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476215939363_899095145","id":"20161009-224054_1063303233","dateCreated":"2016-10-11T07:58:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3074"},{"title":"Clustering on channel category using HashingTF","text":"import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\nimport collection.breakOut\n\nval df = sqlContext.createDataFrame(Seq(\n  (0, \"abc, bac, c\"),\n  (1, \"a, b, b c, a\"),\n  (2, \"a, e, a\"),\n  (3, \"e, f\"),\n  (4, \"a, e, c\"),\n  (5, \"b, a, c\"),\n  (6, \"c, a, b\")\n)).toDF(\"user_id\", \"channels\")\n\nval tokenizer = new Tokenizer().setInputCol(\"channels\").setOutputCol(\"tokenized\")\nval wordsData = tokenizer.transform(df)\nval hashingTF = new HashingTF().setBinary(true).setInputCol(\"tokenized\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\nval featurizedData = hashingTF.transform(wordsData)\nfeaturizedData.select(\"rawFeatures\").show(false)\n\nval idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"extractedFeatures\")\nval idfModel = idf.fit(featurizedData)\nval rescaledData = idfModel.transform(featurizedData)\nval hashedFeatures = rescaledData.select(\"extractedFeatures\")\nhashedFeatures.show(false)\n// The above solution is extremely ugly and needs re-consideration but in interes of time need to move on\nval featureExtraction = hashedFeatures.rdd.map{x => x(0).toString.split(\"],\")(1).\n                                                    replaceAll(\"\\\\)\\\\[\\\\]\",\"\").split(\",\").map(_.trim).toList}\n//val featureExtraction = hashedFeatures.rdd\nfeatureExtraction.take(2)\n\n/*val assembler = new VectorAssembler().setInputCols(Array(\"extractedFeatures\")).setOutputCol(\"features\")\nval model = new KMeans().setSeed(0).setK(2).setMaxIter(10).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\nval pipeline = new Pipeline().setStages(Array(assembler, model))\nval kMeansModelFit = pipeline.fit(rescaledData)\nval transformedFeaturesDF = kMeansModelFit.transform(rescaledData)\ntransformedFeaturesDF.show()\n\nval cost = kMeansModelFit.stages(1).asInstanceOf[KMeansModel].computeCost(transformedFeaturesDF)\nprintln(cost)\nprintln(\"Cluster Centers: \")\nkMeansModelFit.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach(println)\n*/","dateUpdated":"2016-10-11T07:58:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476215939363_899095145","id":"20161001-022039_287104891","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\nimport collection.breakOut\ndf: org.apache.spark.sql.DataFrame = [user_id: int, channels: string]\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_5b0caca5a504\nwordsData: org.apache.spark.sql.DataFrame = [user_id: int, channels: string ... 1 more field]\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_785bc4c6412f\nfeaturizedData: org.apache.spark.sql.DataFrame = [user_id: int, channels: string ... 2 more fields]\n+----------------------------------------+\n|rawFeatures                             |\n+----------------------------------------+\n|(20,[2,7,18],[1.0,1.0,1.0])             |\n|(20,[0,1,7,10,11],[1.0,1.0,1.0,1.0,1.0])|\n|(20,[0,10,12],[1.0,1.0,1.0])            |\n|(20,[8,12],[1.0,1.0])                   |\n|(20,[0,2,12],[1.0,1.0,1.0])             |\n|(20,[0,2,11],[1.0,1.0,1.0])             |\n|(20,[0,1,7],[1.0,1.0,1.0])              |\n+----------------------------------------+\n\nidf: org.apache.spark.ml.feature.IDF = idf_b7d0e135c542\nidfModel: org.apache.spark.ml.feature.IDFModel = idf_b7d0e135c542\nrescaledData: org.apache.spark.sql.DataFrame = [user_id: int, channels: string ... 3 more fields]\nhashedFeatures: org.apache.spark.sql.DataFrame = [extractedFeatures: vector]\n+--------------------------------------------------------------------------------------------------------------------+\n|extractedFeatures                                                                                                   |\n+--------------------------------------------------------------------------------------------------------------------+\n|(20,[2,7,18],[0.6931471805599453,0.6931471805599453,1.3862943611198906])                                            |\n|(20,[0,1,7,10,11],[0.28768207245178085,0.9808292530117262,0.6931471805599453,0.9808292530117262,0.9808292530117262])|\n|(20,[0,10,12],[0.28768207245178085,0.9808292530117262,0.6931471805599453])                                          |\n|(20,[8,12],[1.3862943611198906,0.6931471805599453])                                                                 |\n|(20,[0,2,12],[0.28768207245178085,0.6931471805599453,0.6931471805599453])                                           |\n|(20,[0,2,11],[0.28768207245178085,0.6931471805599453,0.9808292530117262])                                           |\n|(20,[0,1,7],[0.28768207245178085,0.9808292530117262,0.6931471805599453])                                            |\n+--------------------------------------------------------------------------------------------------------------------+\n\nfeatureExtraction: org.apache.spark.rdd.RDD[List[String]] = MapPartitionsRDD[32] at map at <console>:78\nres12: Array[List[String]] = Array(List([0.6931471805599453, 0.6931471805599453, 1.3862943611198906])), List([0.28768207245178085, 0.9808292530117262, 0.6931471805599453, 0.9808292530117262, 0.9808292530117262])))\n"},"dateCreated":"2016-10-11T07:58:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3075"},{"dateUpdated":"2016-10-11T07:58:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476215939364_897171400","id":"20161009-194751_1783312488","dateCreated":"2016-10-11T07:58:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3076"},{"title":"LDA for clustering","text":"import org.apache.spark.ml.clustering.LDA\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n\nval sqlContext = new SQLContext(sc)\n\n// Loads data.\n/*\nval dataset = sqlContext.read.format(\"libsvm\")\n  .load(\"s3://ds-etl/pluto-tv/sample_lda_libsvm_data.txt\")\ndataset.show()\n*/\n\nval df = spark.createDataFrame(Seq(\n  (0, Array(\"a\", \"b\", \"c\")),\n  (1, Array(\"a\", \"b\", \"b\", \"c\", \"a\")),\n  (2, Array(\"a\", \"e\", \"a\")),\n  (3, Array(\"e\", \"f\")),\n  (4, Array(\"b\", \"c\", \"a\"))\n)).toDF(\"id\", \"words\")\n\nval cvModel: CountVectorizerModel = new CountVectorizer()\n  .setInputCol(\"words\")\n  .setOutputCol(\"features\")\n  .setMinDF(1)\n  .fit(df)\nval dataset = cvModel.transform(df).select(\"features\")\ndataset.show(false)\n\n// Trains a LDA model.\nval lda = new LDA().setK(3).setMaxIter(20)\nval model = lda.fit(dataset)\n\nval ll = model.logLikelihood(dataset)\nval lp = model.logPerplexity(dataset)\nprintln(s\"The lower bound on the log likelihood of the entire corpus: $ll\")\nprintln(s\"The upper bound bound on perplexity: $lp\")\n\n// Describe topics.\nval topics = model.describeTopics(4)\nprintln(\"The topics described by their top-weighted terms:\")\ntopics.show(false)\n\n// Shows the result.\nval transformed = model.transform(dataset)\ntransformed.show(false)","dateUpdated":"2016-10-11T07:58:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476215939364_897171400","id":"20160924-014559_990022185","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.clustering.LDA\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\nwarning: there was one deprecation warning; re-run with -deprecation for details\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@5d4abeb1\ndf: org.apache.spark.sql.DataFrame = [id: int, words: array<string>]\ncvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_36dcae8d1e42\ndataset: org.apache.spark.sql.DataFrame = [features: vector]\n+-------------------------+\n|features                 |\n+-------------------------+\n|(5,[0,1,2],[1.0,1.0,1.0])|\n|(5,[0,1,2],[2.0,2.0,1.0])|\n|(5,[0,3],[2.0,1.0])      |\n|(5,[3,4],[1.0,1.0])      |\n|(5,[0,1,2],[1.0,1.0,1.0])|\n+-------------------------+\n\nlda: org.apache.spark.ml.clustering.LDA = lda_fb1d5237e29b\nmodel: org.apache.spark.ml.clustering.LDAModel = lda_fb1d5237e29b\nll: Double = -44.12871187192622\nlp: Double = 2.7580445363699977\nThe lower bound on the log likelihood of the entire corpus: -44.12871187192622\nThe upper bound bound on perplexity: 2.7580445363699977\ntopics: org.apache.spark.sql.DataFrame = [topic: int, termIndices: array<int> ... 1 more field]\nThe topics described by their top-weighted terms:\n+-----+------------+-----------------------------------------------------------------------------------+\n|topic|termIndices |termWeights                                                                        |\n+-----+------------+-----------------------------------------------------------------------------------+\n|0    |[2, 4, 1, 0]|[0.2363086762118094, 0.20830947074058656, 0.194267135074829, 0.18112610523735406]  |\n|1    |[0, 2, 4, 3]|[0.26327353215692834, 0.20910251734177976, 0.1940171993257593, 0.1794967622752859] |\n|2    |[2, 3, 1, 0]|[0.2151592944614562, 0.20486745178377408, 0.19680353536502357, 0.19502863490749087]|\n+-----+------------+-----------------------------------------------------------------------------------+\n\ntransformed: org.apache.spark.sql.DataFrame = [features: vector, topicDistribution: vector]\n+-------------------------+------------------------------------------------------------+\n|features                 |topicDistribution                                           |\n+-------------------------+------------------------------------------------------------+\n|(5,[0,1,2],[1.0,1.0,1.0])|[0.10885338108117794,0.781052319099287,0.11009429981953509] |\n|(5,[0,1,2],[2.0,2.0,1.0])|[0.07041342317897177,0.8568670085259443,0.07271956829508389]|\n|(5,[0,3],[2.0,1.0])      |[0.09345895971814394,0.8080053839630823,0.09853565631877376]|\n|(5,[3,4],[1.0,1.0])      |[0.14276364878506534,0.1516928220549488,0.7055435291599859] |\n|(5,[0,1,2],[1.0,1.0,1.0])|[0.10859185235478958,0.78113586942291,0.11027227822230043]  |\n+-------------------------+------------------------------------------------------------+\n\n"},"dateCreated":"2016-10-11T07:58:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3077"},{"dateUpdated":"2016-10-11T07:58:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476215939364_897171400","id":"20160914-215115_116611523","dateCreated":"2016-10-11T07:58:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3080"}],"name":"kmeans_skeleton","id":"2C17ZREMG","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}