{
  "paragraphs": [
    {
      "text": "%dep\nz.reset()\nz.load(\"com.databricks:spark-csv_2.10:1.5.0\")",
      "dateUpdated": "2016-10-11T09:18:31+0000",
      "config": {
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476220683393_-65731456",
      "id": "20160929-050738_296591728",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "DepInterpreter(%dep) deprecated. Remove dependencies and repositories through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@437b7e0a\n"
      },
      "dateCreated": "2016-10-11T09:18:03+0000",
      "dateStarted": "2016-10-11T09:18:31+0000",
      "dateFinished": "2016-10-11T09:18:36+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:184"
    },
    {
      "text": "import java.io.File\nimport scala.io.Source\n\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd._\nimport org.apache.spark.{SparkContext, SparkConf}\nimport org.apache.spark.sql.hive.HiveContext\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType, TimestampType, LongType, ShortType}\nimport org.apache.spark.mllib.linalg.distributed.{ CoordinateMatrix, MatrixEntry }\n\nval hiveContext = new HiveContext(sc)\nval sqlContext = new SQLContext(sc)",
      "dateUpdated": "2016-11-03T00:32:03+0000",
      "config": {
        "colWidth": 12,
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476220683393_-65731456",
      "id": "20160929-050741_1836334410",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.io.File\nimport scala.io.Source\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd._\nimport org.apache.spark.{SparkContext, SparkConf}\nimport org.apache.spark.sql.hive.HiveContext\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType, TimestampType, LongType, ShortType}\nimport org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\nwarning: there was one deprecation warning; re-run with -deprecation for details\nhiveContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@b507efa\nwarning: there was one deprecation warning; re-run with -deprecation for details\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@77b8251\n"
      },
      "dateCreated": "2016-10-11T09:18:03+0000",
      "dateStarted": "2016-11-03T00:32:03+0000",
      "dateFinished": "2016-11-03T00:32:13+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:185",
      "focus": true
    },
    {
      "text": "/*\nval idf = sqlContext.createDataFrame(Seq(\n  (\"0\", \"11\"),\n  (\"1\", \"12\"),\n  (\"2\", \"12\"),\n  (\"3\", \"13\"),\n  (\"4\", \"14\"),\n  (\"5\", \"15\"),\n  (\"0\", \"11\"),\n  (\"1\", \"22\"),\n  (\"2\", \"12\"),\n  (\"6\", \"13\"),\n  (\"4\", \"14\"),\n  (\"7\", \"15\"),\n  (\"0\", \"22\"),\n  (\"2\", \"22\"),\n  (\"6\", \"23\"),\n  (\"4\", \"24\"),\n  (\"7\", \"25\"),\n  (\"0\", \"26\"),\n  (\"8\", \"11\"),\n  (\"9\", \"12\"),\n  (\"8\", \"22\"),\n  (\"9\", \"11\"),\n  (\"9\", \"26\"),\n  (\"6\", \"12\"),\n  (\"2\", \"23\")\n)).toDF(\"user_id\", \"channel_id\")\n*/\n/*\nval df = sqlContext.createDataFrame(Seq(\n    (\"0\", \"11\"),\n    (\"1\", \"12\"),\n    (\"2\", \"13\"),\n    (\"3\", \"14\"),\n    (\"4\", \"15\"),\n    (\"4\", \"16\"),\n    (\"4\", \"11\"),\n    (\"0\", \"12\"),\n    (\"2\", \"11\"),\n    (\"2\", \"12\"),\n    (\"4\", \"13\")\n    )).toDF(\"user_id\", \"channel_id\") */\n    \nval df = sqlContext.createDataFrame(Seq(\n    (0, \"11\"),\n    (1, \"12\"),\n    (2, \"13\"),\n    (3, \"14\"),\n    (4, \"15\"),\n    (4, \"16\"),\n    (4, \"11\"),\n    (0, \"12\"),\n    (2, \"11\"),\n    (2, \"12\"),\n    (4, \"13\")\n    )).toDF(\"user_id\", \"channel_id\")",
      "dateUpdated": "2016-11-03T16:24:12+0000",
      "config": {
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476220683393_-65731456",
      "id": "20160929-051442_108741282",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame = [user_id: int, channel_id: string]\n"
      },
      "dateCreated": "2016-10-11T09:18:03+0000",
      "dateStarted": "2016-11-03T16:24:12+0000",
      "dateFinished": "2016-11-03T16:24:13+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:186",
      "focus": true
    },
    {
      "text": "// This section is needed only if you have alphanumeric values for defining <user_id and channel_id>\nval userIdIndexer = new StringIndexer()\n    .setInputCol(\"user_id\")\n    .setOutputCol(\"user_id_index\")\nval userIdindexed = userIdIndexer.fit(df).transform(df)\n\nval channelIdIndexer = new StringIndexer()\n    .setInputCol(\"channel_id\")\n    .setOutputCol(\"channel_id_index\")\nval channelIdIndexed = channelIdIndexer.\n                            fit(userIdindexed).\n                            transform(userIdindexed)\nchannelIdIndexed.show(false)",
      "dateUpdated": "2016-11-03T03:19:31+0000",
      "config": {
        "colWidth": 12,
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1478043704405_1020036813",
      "id": "20161101-234144_131471992",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "userIdIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_1a8bf089d3dd\nuserIdindexed: org.apache.spark.sql.DataFrame = [user_id: string, channel_id: string ... 1 more field]\nchannelIdIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_dc1f3a101a85\nchannelIdIndexed: org.apache.spark.sql.DataFrame = [user_id: string, channel_id: string ... 2 more fields]\n+-------+----------+-------------+----------------+\n|user_id|channel_id|user_id_index|channel_id_index|\n+-------+----------+-------------+----------------+\n|0      |11        |2.0          |1.0             |\n|1      |12        |3.0          |0.0             |\n|2      |13        |1.0          |2.0             |\n|3      |14        |4.0          |5.0             |\n|4      |15        |0.0          |3.0             |\n|4      |16        |0.0          |4.0             |\n|4      |11        |0.0          |1.0             |\n|0      |12        |2.0          |0.0             |\n|2      |11        |1.0          |1.0             |\n|2      |12        |1.0          |0.0             |\n|4      |13        |0.0          |2.0             |\n+-------+----------+-------------+----------------+\n\n"
      },
      "dateCreated": "2016-11-01T23:41:44+0000",
      "dateStarted": "2016-11-03T03:19:31+0000",
      "dateFinished": "2016-11-03T03:19:41+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:187",
      "focus": true
    },
    {
      "config": {
        "colWidth": 12,
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1478049822857_-429178262",
      "id": "20161102-012342_1848430671",
      "dateCreated": "2016-11-02T01:23:42+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:188",
      "dateUpdated": "2016-11-03T00:51:42+0000",
      "dateFinished": "2016-11-03T00:51:43+0000",
      "dateStarted": "2016-11-03T00:51:42+0000",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "root\n |-- user_id: string (nullable = true)\n |-- channel_id: string (nullable = true)\n |-- user_id_index: double (nullable = true)\n |-- channel_id_index: double (nullable = true)\n\n"
      },
      "text": "channelIdIndexed.printSchema",
      "focus": true
    },
    {
      "text": "val channelIdChannelIndexDf = channelIdIndexed.select(\"channel_id_index\", \"channel_id\").distinct\nval i = 0\nprintln(channelIdChannelIndexDf.filter(s\"channel_id_index == ${i}\").select(\"channel_id\").first()(0))",
      "dateUpdated": "2016-11-02T06:36:41+0000",
      "config": {
        "colWidth": 12,
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1478048060191_871376477",
      "id": "20161102-005420_758985246",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "channelIdChannelIndexDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [channel_id_index: double, channel_id: string]\ni: Int = 0\n12\n"
      },
      "dateCreated": "2016-11-02T00:54:20+0000",
      "dateStarted": "2016-11-02T06:36:41+0000",
      "dateFinished": "2016-11-02T06:36:48+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:189",
      "focus": true
    },
    {
      "config": {
        "colWidth": 12,
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1478190278702_132524574",
      "id": "20161103-162438_1649324706",
      "dateCreated": "2016-11-03T16:24:38+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "focus": true,
      "$$hashKey": "object:1967",
      "text": "case class ChannelAffinity(userId:Long, itemId:Long, affinity:Double)\n\nval userChannelRdd = df.select(\"user_id\", \"channel_id\").\n    rdd.map(x => x(0) + \"\\t\" + x(1))\n\nval userToItem = userChannelRdd.map(_.split(\"\\t\").toSeq)\n\nval userToItemAffinity = userToItem.map(x => (x(0).toDouble,x(1).toDouble)).map(y => (y,1)).reduceByKey(_ + _).map {\n      case ((userId, itemId), affinityScore) => ChannelAffinity (userId.toLong, itemId.toLong, affinityScore)\n    }\n    \nuserToItemAffinity.toDF.show(false)",
      "dateUpdated": "2016-11-03T16:26:01+0000",
      "dateFinished": "2016-11-03T16:26:05+0000",
      "dateStarted": "2016-11-03T16:26:01+0000",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class ChannelAffinity\nuserChannelRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1246] at map at <console>:301\nuserToItem: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[1247] at map at <console>:302\nuserToItemAffinity: org.apache.spark.rdd.RDD[ChannelAffinity] = MapPartitionsRDD[1251] at map at <console>:306\n+------+------+--------+\n|userId|itemId|affinity|\n+------+------+--------+\n|3     |14    |1.0     |\n|4     |15    |1.0     |\n|2     |11    |1.0     |\n|2     |13    |1.0     |\n|1     |12    |1.0     |\n|0     |11    |1.0     |\n|4     |11    |1.0     |\n|4     |16    |1.0     |\n|0     |12    |1.0     |\n|2     |12    |1.0     |\n|4     |13    |1.0     |\n+------+------+--------+\n\n"
      }
    },
    {
      "text": "case class ChannelAffinity(userId:Long, itemId:Long, affinity:Double)\n\nval userChannelRdd = channelIdIndexed.select(\"user_id_index\", \"channel_id_index\").\n    rdd.map(x => x.getDouble(0) + \"\\t\" + x.getDouble(1))\n\nval userToItem = userChannelRdd.map(_.split(\"\\t\").toSeq)\n\nval userToItemAffinity = userToItem.map(x => (x(0).toDouble,x(1).toDouble)).map(y => (y,1)).reduceByKey(_ + _).map {\n      case ((userId, itemId), affinityScore) => ChannelAffinity (userId.toLong, itemId.toLong, affinityScore)\n    }\n    \nuserToItemAffinity.toDF.show(false)",
      "dateUpdated": "2016-11-03T03:05:45+0000",
      "config": {
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476220683393_-65731456",
      "id": "20160929-052044_136857306",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class ChannelAffinity\nuserChannelRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1157] at map at <console>:309\nuserToItem: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[1158] at map at <console>:310\nuserToItemAffinity: org.apache.spark.rdd.RDD[ChannelAffinity] = MapPartitionsRDD[1162] at map at <console>:314\n+------+------+--------+\n|userId|itemId|affinity|\n+------+------+--------+\n|2     |1     |1.0     |\n|0     |4     |1.0     |\n|0     |2     |1.0     |\n|1     |0     |1.0     |\n|3     |0     |1.0     |\n|0     |1     |1.0     |\n|4     |5     |1.0     |\n|1     |1     |1.0     |\n|1     |3     |1.0     |\n|2     |0     |1.0     |\n+------+------+--------+\n\n"
      },
      "dateCreated": "2016-10-11T09:18:03+0000",
      "dateStarted": "2016-11-03T03:05:45+0000",
      "dateFinished": "2016-11-03T03:05:50+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:191",
      "focus": true
    },
    {
      "text": "val userItemMatrix = new CoordinateMatrix(userToItemAffinity.map {\n      case ChannelAffinity(userId, itemId, affinity) => MatrixEntry(userId, itemId, affinity)\n    })\nval itemSimilarities = userItemMatrix.toRowMatrix.columnSimilarities()\n//TODO: figure out a way to print the contents of a item similarity matrix\n\nval result = itemSimilarities.entries.map {\n      case MatrixEntry(item1, item2, cosineSimilarity) => ((item1, item2), cosineSimilarity)\n    }\n    \n// display the result\nval affinityDf = result.toDF().selectExpr(\"_1 channel_pairs\", \"_2 affinity\")\naffinityDf.show(false)",
      "dateUpdated": "2016-11-03T16:39:50+0000",
      "config": {
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476220683393_-65731456",
      "id": "20160929-052637_1144726711",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "userItemMatrix: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix = org.apache.spark.mllib.linalg.distributed.CoordinateMatrix@683d297\nitemSimilarities: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix = org.apache.spark.mllib.linalg.distributed.CoordinateMatrix@69ba70fe\nresult: org.apache.spark.rdd.RDD[((Long, Long), Double)] = MapPartitionsRDD[1265] at map at <console>:312\naffinityDf: org.apache.spark.sql.DataFrame = [channel_pairs: struct<_1: bigint, _2: bigint>, affinity: double]\n+-------------+------------------+\n|channel_pairs|affinity          |\n+-------------+------------------+\n|[15,16]      |1.0               |\n|[11,15]      |0.5773502691896258|\n|[13,15]      |0.7071067811865475|\n|[13,16]      |0.7071067811865475|\n|[12,13]      |0.408248290463863 |\n|[11,12]      |0.6666666666666669|\n|[11,16]      |0.5773502691896258|\n|[11,13]      |0.816496580927726 |\n+-------------+------------------+\n\n"
      },
      "dateCreated": "2016-10-11T09:18:03+0000",
      "dateStarted": "2016-11-03T16:39:50+0000",
      "dateFinished": "2016-11-03T16:40:01+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:192",
      "focus": true
    },
    {
      "dateUpdated": "2016-11-02T06:57:46+0000",
      "config": {
        "colWidth": 12,
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476220683393_-65731456",
      "id": "20160929-053504_644401602",
      "dateCreated": "2016-10-11T09:18:03+0000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:193",
      "dateFinished": "2016-11-02T06:39:09+0000",
      "dateStarted": "2016-11-02T06:39:09+0000",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res263: org.apache.spark.rdd.RDD[Any] = MapPartitionsRDD[585] at map at <console>:157\n"
      },
      "text": "// Re-arrange the output for better read-ability\n// affinityDf.select($\"channel_pairs._1\", $\"channel_pairs._2\").rdd.map(x => (x.getLong(0), x.getLong(1))).\n//    map(y => channelIdChannelIndexDf.filter(s\"channel_id_index == ${y._1}\").select(\"channel_id\").first()(0))\n\naffinityDf.select($\"channel_pairs._1\", $\"channel_pairs._2\", $\"afffinity\").rdd.map(x => (x.getLong(0), (x.getLong(1), x.getLong(2))))",
      "focus": true
    },
    {
      "config": {
        "colWidth": 12,
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1478064826122_1728649724",
      "id": "20161102-053346_1306170193",
      "dateCreated": "2016-11-02T05:33:46+0000",
      "status": "READY",
      "progressUpdateIntervalMs": 500,
      "focus": true,
      "$$hashKey": "object:1134",
      "dateUpdated": "2016-11-02T20:09:36+0000",
      "text": ""
    }
  ],
  "name": "item_item_collaborative_filtering",
  "id": "2BXS9TDVS",
  "angularObjects": {
    "2BRWU4WXC:shared_process": [],
    "2AM1YV5CU:shared_process": [],
    "2AJXGMUUJ:shared_process": [],
    "2ANGGHHMQ:shared_process": [],
    "2AKK3QQXU:shared_process": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}
